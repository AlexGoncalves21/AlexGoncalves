{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makemore part 1: solved exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex1 - Making it a Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for .., target is e\n",
      "for .e, target is x\n",
      "for ex, target is e\n",
      "for xe, target is m\n",
      "for em, target is p\n",
      "for mp, target is l\n",
      "for pl, target is o\n",
      "for lo, target is .\n"
     ]
    }
   ],
   "source": [
    "def groupbytwo(str):\n",
    "    list = []\n",
    "    if len(str) != 0:\n",
    "        ntimes = len(str) - 1\n",
    "    while ntimes != 0:\n",
    "        list.append(str[:2])\n",
    "        str = str[1:]\n",
    "        ntimes -= 1\n",
    "    return list\n",
    "\n",
    "chs = '..' + 'exemplo' + '.'\n",
    "for ch1, ch2 in zip(groupbytwo(chs), chs[2:]):\n",
    "    print(f'for {ch1}, target is {ch2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = '..' + w + '.'\n",
    "  for ch1, ch2 in zip(groupbytwo(chs), chs[2:]):\n",
    "    ix1 = [stoi[ch1[0]],stoi[ch1[1]]]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = ys.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242241382598877\n",
      "3.512132167816162\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for k in range(2):\n",
    "  \n",
    "  # forward pass\n",
    "  #xenc = F.one_hot(xs, num_classes=it+1).float() # input to the network: one-hot encoding\n",
    "  xenc = torch.zeros(num,27*2)\n",
    "  for line in range(num): xenc[line] = torch.from_numpy(np.concatenate([F.one_hot(xs[line][0], num_classes = 27).float(), F.one_hot(xs[line][1], num_classes = 27).float()],axis = 0)) \n",
    "\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50* W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krklxqaltx.\n",
      "bemkahodzik.\n",
      "rkch.\n",
      "ejaes.\n",
      "rllonmcwx.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(123456)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0 #int corresponding to '.'\n",
    "  previous = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = torch.from_numpy(np.concatenate([previous,F.one_hot(torch.tensor([ix]), num_classes=27).float()], axis=1))\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    previous = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if out[-1][-1] == '.':\n",
    "      break\n",
    "    \n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2 - Solve previous ex without hardcoding context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for ..., target is e\n",
      "for ..e, target is x\n",
      "for .ex, target is e\n",
      "for exe, target is m\n",
      "for xem, target is p\n",
      "for emp, target is l\n",
      "for mpl, target is o\n",
      "for plo, target is .\n"
     ]
    }
   ],
   "source": [
    "def groupbyn(str,n):\n",
    "    list = []\n",
    "    if len(str) != 0:\n",
    "        ntimes = len(str) - 1\n",
    "    while ntimes != 0:\n",
    "        list.append(str[:n])\n",
    "        str = str[1:]\n",
    "        ntimes -= 1\n",
    "    return list\n",
    "\n",
    "n = 3\n",
    "chs = 'exemplo'\n",
    "\n",
    "for i in range(n): chs = '.' + chs\n",
    "chs = chs +'.'\n",
    "for ch1, ch2 in zip(groupbyn(chs,n), chs[n:]):\n",
    "    print(f'for {ch1}, target is {ch2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 3 # context size\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "\n",
    "  chs = w\n",
    "  for i in range(n): chs = '.' + chs\n",
    "  chs = chs +'.'\n",
    "\n",
    "  for ch1, ch2 in zip(groupbyn(chs,n), chs[n:]):\n",
    "    ix1 = []\n",
    "    for i in ch1: ix1.append(stoi[i])\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = ys.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*n, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.522070407867432\n",
      "4.238542556762695\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for k in range(10):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = torch.zeros(num,27*n)\n",
    "  \n",
    "  for line in range(num): \n",
    "    toconcat = []\n",
    "    for i in range(n): toconcat.append(F.one_hot(xs[line][i], num_classes = 27).float())\n",
    "    xenc[line] = torch.from_numpy(np.concatenate(toconcat, axis = 0)) \n",
    "\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50* W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kelxlpzgdkdqhfjibihudqgayna.\n",
      "eeein.\n",
      "tre.\n",
      "rryhovo.\n",
      "ecschzxt.\n",
      "eesrrewfryiadyckdckchucbch.\n",
      "err.\n",
      "eilgt.\n",
      "irr.\n",
      "eerkomnzcgmqza.\n"
     ]
    }
   ],
   "source": [
    "# Sample some examples\n",
    "\n",
    "g = torch.Generator().manual_seed(1234)\n",
    "\n",
    "for i in range(10):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0 #int corresponding to '.'\n",
    "  previous = []\n",
    "  for i in range(n-1): previous.append(F.one_hot(torch.tensor([ix]), num_classes=27).float())\n",
    "\n",
    "  while True:\n",
    "    previous.append(F.one_hot(torch.tensor([ix]), num_classes=27).float())\n",
    "    xenc = torch.from_numpy(np.concatenate(previous, axis=1))\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    previous.pop(0)\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if out[-1][-1] == '.':\n",
    "      break\n",
    "    \n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
