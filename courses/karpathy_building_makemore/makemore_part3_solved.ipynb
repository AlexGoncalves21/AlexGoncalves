{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex1 -  I did not get around to seeing what happens when you initialize all weights and biases to zero. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.7348735332489014\n",
      "\n",
      "C:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "W1:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "W2:\n",
      "tensor([[-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005],\n",
      "        [-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005],\n",
      "        [-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005],\n",
      "        ...,\n",
      "        [-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005],\n",
      "        [-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005],\n",
      "        [-0.0064,  0.0038,  0.0005,  ..., -0.0012,  0.0005,  0.0005]])\n",
      "\n",
      "b2:\n",
      "tensor([-0.1433,  0.0853,  0.0116,  0.0153, -0.0381, -0.0649,  0.0040,  0.0083,\n",
      "         0.0340,  0.0764,  0.0131,  0.0225,  0.0299, -0.0022,  0.0178, -0.1201,\n",
      "         0.0046,  0.0012,  0.0232, -0.0268,  0.0246,  0.0136,  0.0113,  0.0043,\n",
      "        -0.0281,  0.0120,  0.0103])\n",
      "\n",
      "bngain:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "bnbias:\n",
      "tensor([[-0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.zeros(vocab_size,n_embd)#torch.randn((vocab_size, n_embd),            generator=g)\n",
    "#C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.zeros(n_embd * block_size, n_hidden)\n",
    "W2 = torch.zeros(n_hidden, vocab_size)\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.zeros((1, n_hidden))\n",
    "bnbias = torch.ones((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / (bnstdi + 1e-05)**0.5 + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters: p.data += -lr * p.grad\n",
    "  \n",
    "  names = ['C', 'W1', 'W2', 'b2', 'bngain', 'bnbias']\n",
    "  count = 0\n",
    "  \n",
    "  if i == max_steps-1:\n",
    "    print(\"loss: \"+str(loss.item()))\n",
    "    print(\"\")\n",
    "    for p in parameters:\n",
    "      print(names[count]+\":\")\n",
    "      print(p.grad)\n",
    "      print(\"\")\n",
    "      count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we imagine all weights to be zero, then that would mean that, regardless of the input, all layers would have a pre activation equal to the bias. Seeing as though the bias are also initialized as zero and f(0) = 0 on the tanh function. Therefore, for every hidden layer, both the input and the output are zero.\n",
    "When we apply backpropagation, we determine the impact of a certain parameter on the final result by calculating the derivative of the error with respect to that parameter. Considering that, when everything else remains constant, changing any of the parameters leads to the same result, we conclude that the gradient for that parameter is zero, and its value will remain the same.\n",
    "It is important to note that this thought process isn't valid when we consider activation functions where f(0)!=0 such as the sigmoid function. In this scenario, we expect the values to change equaly in each layer and also to have all equal collumns (this makes sense if we recall that all layers will have the same input and output).\n",
    "After analysing the results, we note that there is some slow learning hapening on the second layer, as well as in the batchnorm layer. I suspect this is happening because 1e-05 is added on the batch norm layer, in order to prevent division by zero. This makes it so that some of the inputs are not actually zero, but a very small value that enables learning from that point on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex2 - BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural net the standard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  10000: 3.2371\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) # Kunning innit\n",
    "W2 = torch.randn((n_hidden, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) # Kunning innit\n",
    "W3 = torch.randn((n_hidden, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) # Kunning innit\n",
    "\n",
    "W4 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b4 = torch.zeros(vocab_size)\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain1 = torch.zeros((1, n_hidden))\n",
    "bngain2 = torch.zeros((1, n_hidden))\n",
    "bngain3 = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnbias1 = torch.ones((1, n_hidden))\n",
    "bnbias2 = torch.ones((1, n_hidden))\n",
    "bnbias3 = torch.ones((1, n_hidden))\n",
    "\n",
    "bnmean_running1 = torch.zeros((1, n_hidden))\n",
    "bnmean_running2 = torch.zeros((1, n_hidden))\n",
    "bnmean_running3 = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnstd_running1 = torch.ones((1, n_hidden))\n",
    "bnstd_running2 = torch.ones((1, n_hidden))\n",
    "bnstd_running3 = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, W3, W4, b4, bngain1, bngain2, bngain3, bnbias1, bnbias2, bnbias3]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # Forward pass\n",
    "\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "  # Layer 1\n",
    "\n",
    "  hpreact1 = embcat @ W1 # hidden layer pre-activation\n",
    "  bnmeani1 = hpreact1.mean(0, keepdim=True)\n",
    "  bnstdi1 = hpreact1.std(0, keepdim=True)\n",
    "  hpreact1 = bngain1 * (hpreact1 - bnmeani1) / (bnstdi1 + 1e-05)**0.5 + bnbias1\n",
    "  with torch.no_grad():\n",
    "    bnmean_running1 = 0.999 * bnmean_running1 + 0.001 * bnmeani1\n",
    "    bnstd_running1 = 0.999 * bnstd_running1 + 0.001 * bnstdi1\n",
    "  h1 = torch.tanh(hpreact1)\n",
    "\n",
    "  # Layer 2\n",
    "\n",
    "  hpreact2 = h1 @ W2 # hidden layer pre-activation\n",
    "  bnmeani2 = hpreact2.mean(0, keepdim=True)\n",
    "  bnstdi2 = hpreact2.std(0, keepdim=True)\n",
    "  hpreact2 = bngain2 * (hpreact2 - bnmeani2) / (bnstdi2 + 1e-05)**0.5 + bnbias2\n",
    "  with torch.no_grad():\n",
    "    bnmean_running2 = 0.999 * bnmean_running2 + 0.001 * bnmeani2\n",
    "    bnstd_running2 = 0.999 * bnstd_running2 + 0.001 * bnstdi2\n",
    "  h2 = torch.tanh(hpreact2)\n",
    "  \n",
    "  # Layer 3\n",
    "\n",
    "  hpreact3 = h2 @ W3 # hidden layer pre-activation\n",
    "  bnmeani3 = hpreact3.mean(0, keepdim=True)\n",
    "  bnstdi3 = hpreact3.std(0, keepdim=True)\n",
    "  hpreact3 = bngain3 * (hpreact3 - bnmeani3) / (bnstdi3 + 1e-05)**0.5 + bnbias3\n",
    "  with torch.no_grad():\n",
    "    bnmean_running3 = 0.999 * bnmean_running3 + 0.001 * bnmeani3\n",
    "    bnstd_running3 = 0.999 * bnstd_running3 + 0.001 * bnstdi3\n",
    "  h3 = torch.tanh(hpreact3)\n",
    "\n",
    "  # Output Layer\n",
    "\n",
    "  logits = h3 @ W4 + b4\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  #backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference the \"usual\" way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (1,),generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    " \n",
    "# Forward pass\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Layer 1\n",
    "\n",
    "hpreact1 = embcat @ W1 # hidden layer pre-activation\n",
    "bnmeani1 = bnmean_running1\n",
    "bnstdi1 = bnstd_running1\n",
    "hpreact1 = bngain1 * (hpreact1 - bnmeani1) / (bnstdi1 + 1e-05)**0.5 + bnbias1\n",
    "h1 = torch.tanh(hpreact1)\n",
    "\n",
    "# Layer 2\n",
    "\n",
    "hpreact2 = h1 @ W2 # hidden layer pre-activation\n",
    "bnmeani2 = bnmean_running2\n",
    "bnstdi2 = bnstd_running2\n",
    "hpreact2 = bngain2 * (hpreact2 - bnmeani2) / (bnstdi2 + 1e-05)**0.5 + bnbias2\n",
    "h2 = torch.tanh(hpreact2)\n",
    "  \n",
    "# Layer 3\n",
    "\n",
    "hpreact3 = h2 @ W3 # hidden layer pre-activation\n",
    "bnmeani3 = bnmean_running3\n",
    "bnstdi3 = bnstd_running3\n",
    "hpreact3 = bngain3 * (hpreact3 - bnmeani3) / (bnstdi3 + 1e-05)**0.5 + bnbias3\n",
    "h3 = torch.tanh(hpreact3)\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "logits = h3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with the \"folding\" of the batch norm layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "# Layer 1\n",
    "\n",
    "bnmeani1 = bnmean_running1\n",
    "bnstdi1 = bnstd_running1\n",
    "W12 = bngain1 * W1 / (bnstdi1 + 1e-05)**0.5\n",
    "b12 = bngain1 * (-bnmeani1) / (bnstdi1 + 1e-05)**0.5\n",
    "hpreact1 = embcat @ W12 + b12 # hidden layer pre-activation\n",
    "newh1 = torch.tanh(hpreact1)\n",
    "\n",
    "# Layer 2\n",
    "\n",
    "bnmeani2 = bnmean_running2\n",
    "bnstdi2 = bnstd_running2\n",
    "W22 = bngain2 * W2 / (bnstdi2 + 1e-05)**0.5\n",
    "b22 = bngain2 * (-bnmeani2) / (bnstdi2 + 1e-05)**0.5\n",
    "hpreact2 = newh1 @ W22 + b22 # hidden layer pre-activation\n",
    "newh2 = torch.tanh(hpreact2)\n",
    "\n",
    "  \n",
    "# Layer 3\n",
    "\n",
    "bnmeani3 = bnmean_running3\n",
    "bnstdi3 = bnstd_running3\n",
    "W32 = bngain3 * W3 / (bnstdi3 + 1e-05)**0.5\n",
    "b32 = bngain3 * (-bnmeani3) / (bnstdi3 + 1e-05)**0.5\n",
    "hpreact3 = newh2 @ W32 + b32 # hidden layer pre-activation\n",
    "newh3 = torch.tanh(hpreact3)\n",
    "\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "newlogits = h3 @ W4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits == newlogits.all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
